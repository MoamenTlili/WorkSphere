{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58TaJgMxcrdz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Note: This notebook is designed to run in Google Colab.\n",
        "# Data paths refer to Google Drive mount points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXZ-kLRRE_Mr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-VkrfEWdFX-"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/sample_data/multilingual_hate_speech_test.csv', encoding='utf-8')\n",
        "print(df_train['label'].value_counts())\n",
        "df_test = pd.read_csv('/content/sample_data/combined_test_dataset.csv', encoding='utf-8')\n",
        "\n",
        "train_df, val_df = train_test_split(df_train, test_size=0.2, random_state=42)\n",
        "\n",
        "train_texts = train_df['text'].values\n",
        "train_labels = train_df['label'].apply(lambda x: 1 if x == 'HOF' else 0).values\n",
        "val_texts = val_df['text'].values\n",
        "val_labels = val_df['label'].apply(lambda x: 1 if x == 'HOF' else 0).values\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "print(f\"Class weights: {class_weight_dict}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKi9bodQdH80"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/LaBSE')\n",
        "labse_model = TFAutoModel.from_pretrained('sentence-transformers/LaBSE')\n",
        "\n",
        "def set_trainable_layers(model, num_trainable_layers):\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Unfreezed the top layers (including the classifier head)\n",
        "    for layer in model.layers[-num_trainable_layers:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "# unfreezed last 8 layers\n",
        "set_trainable_layers(labse_model, num_trainable_layers=8)\n",
        "print(\"\\nTrainable Layers Status:\")\n",
        "for i, layer in enumerate(labse_model.layers[-10:]):\n",
        "    print(f\"Layer {len(labse_model.layers)-10+i}: {layer.name} -> {layer.trainable}\")\n",
        "\n",
        "def preprocess_data(texts, tokenizer, max_length=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for text in texts:\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        input_ids.append(encoding['input_ids'])\n",
        "        attention_masks.append(encoding['attention_mask'])\n",
        "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
        "\n",
        "train_input_ids, train_attention_mask = preprocess_data(train_texts, tokenizer)\n",
        "val_input_ids, val_attention_mask = preprocess_data(val_texts, tokenizer)\n",
        "\n",
        "class F1ScoreCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data):\n",
        "        self.validation_data = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
        "        val_targ = self.validation_data[1]\n",
        "        _val_f1 = f1_score(val_targ, val_predict)\n",
        "        print(f\" --- val_f1: {_val_f1}\")\n",
        "        logs['val_f1'] = _val_f1\n",
        "\n",
        "def focal_loss(gamma=2., alpha=0.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1. - epsilon)\n",
        "        pt = tf.where(tf.keras.backend.equal(y_true, 1), y_pred, 1 - y_pred)\n",
        "        return -tf.keras.backend.sum(alpha * tf.keras.backend.pow(1. - pt, gamma) * tf.keras.backend.log(pt))\n",
        "    return focal_loss_fixed\n",
        "\n",
        "class LaBSEWrapper(tf.keras.layers.Layer):\n",
        "    def __init__(self, labse_model):\n",
        "        super(LaBSEWrapper, self).__init__()\n",
        "        self.labse_model = labse_model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, attention_mask = inputs\n",
        "        outputs = self.labse_model(input_ids, attention_mask=attention_mask)\n",
        "        return outputs.last_hidden_state\n",
        "\n",
        "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32)\n",
        "attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32)\n",
        "\n",
        "labse_wrapper = LaBSEWrapper(labse_model)\n",
        "embeddings = labse_wrapper([input_ids, attention_mask])\n",
        "cls_embedding = embeddings[:, 0, :]\n",
        "\n",
        "hidden_layer = tf.keras.layers.Dense(250, activation='relu', kernel_regularizer=regularizers.l2(0.01))(cls_embedding)\n",
        "dropout_layer = tf.keras.layers.Dropout(0.4)(hidden_layer)\n",
        "hidden_layer2 = tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.01))(dropout_layer)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_layer2)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "embedding_model = tf.keras.Model(\n",
        "    inputs=[input_ids, attention_mask],\n",
        "    outputs=cls_embedding\n",
        ")\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "              loss=focal_loss(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNaRPesadMBc"
      },
      "outputs": [],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "f1_callback = F1ScoreCallback(validation_data=([val_input_ids, val_attention_mask], val_labels))\n",
        "\n",
        "history = model.fit(\n",
        "    [train_input_ids, train_attention_mask],\n",
        "    train_labels,\n",
        "    epochs=70,\n",
        "    batch_size=32,\n",
        "    validation_data=([val_input_ids, val_attention_mask], val_labels),\n",
        "    callbacks=[f1_callback, early_stopping],\n",
        "    class_weight=class_weight_dict\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkmmfdEWKfXa"
      },
      "outputs": [],
      "source": [
        "val_predictions_prob = model.predict([val_input_ids, val_attention_mask]).flatten()\n",
        "thresholds = np.arange(0.1, 1.0, 0.1)\n",
        "f1_scores = []\n",
        "for threshold in thresholds:\n",
        "    binary_predictions = (val_predictions_prob >= threshold).astype(int)\n",
        "    f1 = f1_score(val_labels, binary_predictions)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "best_threshold_index = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_threshold_index]\n",
        "print(f\"\\nBest Threshold: {best_threshold:.2f} with F1 Score: {f1_scores[best_threshold_index]:.4f}\")\n",
        "\n",
        "val_predictions = (val_predictions_prob >= best_threshold).astype(int)\n",
        "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
        "val_loss = tf.keras.losses.binary_crossentropy(val_labels, val_predictions_prob).numpy().mean()\n",
        "print(f\"Validation Accuracy (Best Threshold): {val_accuracy:.4f}\")\n",
        "print(f\"Validation Loss (Best Threshold): {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3N9AB6fKnWR"
      },
      "outputs": [],
      "source": [
        "test_texts = df_test['text'].values\n",
        "test_labels = df_test['label'].apply(lambda x: 1 if x == 'HOF' else 0).values\n",
        "test_input_ids, test_attention_mask = preprocess_data(test_texts, tokenizer)\n",
        "\n",
        "test_predictions_prob = model.predict([test_input_ids, test_attention_mask]).flatten()\n",
        "test_predictions = (test_predictions_prob >= best_threshold).astype(int)\n",
        "\n",
        "f1 = f1_score(test_labels, test_predictions)\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "\n",
        "print(\"\\nClassification Report (Test):\")\n",
        "print(classification_report(test_labels, test_predictions, target_names=['NOT', 'HOF']))\n",
        "print(f\"F1 Score (Test): {f1:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2EPgCf-vvSE"
      },
      "outputs": [],
      "source": [
        "test_texts = df_test['text'].values\n",
        "test_labels = df_test['label'].apply(lambda x: 1 if x == 'HOF' else 0).values\n",
        "test_input_ids, test_attention_mask = preprocess_data(test_texts, tokenizer)\n",
        "\n",
        "test_predictions_prob = model.predict([test_input_ids, test_attention_mask]).flatten()\n",
        "test_predictions = (test_predictions_prob >= best_threshold).astype(int)\n",
        "\n",
        "f1_micro = f1_score(test_labels, test_predictions, average='micro')\n",
        "f1_macro = f1_score(test_labels, test_predictions, average='macro')\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "\n",
        "print(\"\\nClassification Report (Test):\")\n",
        "print(classification_report(test_labels, test_predictions, target_names=['NOT', 'HOF']))\n",
        "print(f\"F1 Score (Micro - Test): {f1_micro:.4f}\")\n",
        "print(f\"F1 Score (Macro - Test): {f1_macro:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WabZQ-EdGHXq"
      },
      "outputs": [],
      "source": [
        "test_samples = [\n",
        "    \"This team is amazing!\",                     # Should be NOT (0)\n",
        "    \"I hate you, you're worthless!\",             # Should be HOF (1)\n",
        "    \"Let's discuss the project tomorrow\",        # Neutral (0)\n",
        "    \"People like you should die\",                # Extreme hate (1)\n",
        "    \"What an idiot!\",                            # Insult (likely 1)\n",
        "    \"The weather is nice today\",                  # Neutral (0)\n",
        "\n",
        "    \"هذا الفريق رائع جدًا!\",                    # Should be NOT (0)\n",
        "    \"أكرهك، أنت لا تساوي شيئًا!\",               # Should be HOF (1)\n",
        "    \"دعنا نناقش المشروع غدًا\",                  # Neutral (0)\n",
        "    \"أمثالك يجب أن يُقتَلوا\",                   # Extreme hate (1)\n",
        "    \"يا غبي!\",                                  # Insult (likely 1)\n",
        "    \"الجو جميل اليوم\",                          # Neutral (0)\n",
        "\n",
        "    \"Cette équipe est incroyable !\",             # Should be NOT (0)\n",
        "    \"Je te déteste, tu ne vaux rien !\",         # Should be HOF (1)\n",
        "    \"On parle du projet demain ?\",              # Neutral (0)\n",
        "    \"Des gens comme toi méritent de mourir\",    # Extreme hate (1)\n",
        "    \"Quel imbécile !\",                          # Insult (likely 1)\n",
        "    \"Il fait beau aujourd'hui\",                 # Neutral (0)\n",
        "\n",
        "    \"أنت عار على المجتمع\",                     # Hate (1)\n",
        "    \"كلنا بشر ويجب أن نحترم بعضنا البعض\",      # Positive (0)\n",
        "    \"أنت مجرد نكرة لا قيمة لك\",                # Insult (1)\n",
        "    \"سأذهب للتسوق في المساء\",                  # Neutral (0)\n",
        "    \"لا مكان لأمثالك بيننا\",                   # Hate (1)\n",
        "    \"مباراة اليوم كانت ممتعة جدًا\",            # Positive (0)\n",
        "\n",
        "    \"Tu es une honte pour l'humanité\",          # Hate (1)\n",
        "    \"Tout le monde mérite du respect\",          # Positive (0)\n",
        "    \"T'es qu'un bon à rien\",                    # Insult (1)\n",
        "    \"Je vais cuisiner ce soir\",                 # Neutral (0)\n",
        "    \"On ne veut pas de toi ici\",                # Hate (1)\n",
        "    \"Ce film était vraiment génial\",            # Positive (0)\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL VALIDATION TEST (REAL EXAMPLES)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for text in test_samples:\n",
        "    # Preprocess\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"tf\",\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    prob = model.predict([inputs[\"input_ids\"], inputs[\"attention_mask\"]])[0][0]\n",
        "    prediction = \"HOF\" if prob >= best_threshold else \"NOT\"\n",
        "\n",
        "    color = \"\\033[91m\" if prediction == \"HOF\" else \"\\033[92m\"\n",
        "    print(f\"{color}Text: {text[:50]}{'...' if len(text)>50 else ''}\")\n",
        "    print(f\"Prediction: {prediction} ({prob:.4f})\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6_XwQTbGR3p"
      },
      "outputs": [],
      "source": [
        "# Save and export\n",
        "export_path = \"/content/drive/MyDrive/WorkSphere_labse_Model_best\"\n",
        "\n",
        "@tf.function(input_signature=[\n",
        "    tf.TensorSpec(shape=[None, 128], dtype=tf.int32, name=\"input_ids\"),\n",
        "    tf.TensorSpec(shape=[None, 128], dtype=tf.int32, name=\"attention_mask\")\n",
        "])\n",
        "def serve(input_ids, attention_mask):\n",
        "    return {\"prediction\": model([input_ids, attention_mask])}\n",
        "\n",
        "tf.saved_model.save(\n",
        "    model,\n",
        "    export_path,\n",
        "    signatures={\n",
        "        'serving_default': serve.get_concrete_function(\n",
        "            input_ids=tf.TensorSpec(shape=[None, 128], dtype=tf.int32),\n",
        "            attention_mask=tf.TensorSpec(shape=[None, 128], dtype=tf.int32)\n",
        "        )\n",
        "    }\n",
        ")\n",
        "\n",
        "tokenizer.save_pretrained(export_path + \"_tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ywdcdu6dRnv"
      },
      "outputs": [],
      "source": [
        "loaded_model = tf.saved_model.load(export_path)\n",
        "infer = loaded_model.signatures['serving_default']\n",
        "\n",
        "test_text = \"This is a test sentence\"\n",
        "inputs = tokenizer(test_text, return_tensors='tf', padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "output = infer(\n",
        "    input_ids=inputs['input_ids'],\n",
        "    attention_mask=inputs['attention_mask']\n",
        ")\n",
        "print(f\"Input: '{test_text}'\\nPrediction: {output['prediction'].numpy()[0][0]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfjEv7w7emeA"
      },
      "outputs": [],
      "source": [
        "def evaluate_prediction(text):\n",
        "    inputs = tokenizer(text,\n",
        "                     return_tensors='tf',\n",
        "                     padding='max_length',\n",
        "                     truncation=True,\n",
        "                     max_length=128)\n",
        "\n",
        "    output = infer(\n",
        "        input_ids=inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask']\n",
        "    )\n",
        "\n",
        "    pred = output['prediction'].numpy()[0][0]\n",
        "    interpretation = \"Hateful\" if pred > 0.7 else \"Safe\" if pred < 0.3 else \"Uncertain\"\n",
        "\n",
        "    print(f\"\"\"\n",
        "    Text: {text}\n",
        "    Raw score: {pred:.4f}\n",
        "    Interpretation: {interpretation}\n",
        "    Confidence: {'High' if abs(pred-0.5)>0.4 else 'Medium' if abs(pred-0.5)>0.2 else 'Low'}\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "evaluate_prediction(\"You're amazing!\")\n",
        "evaluate_prediction(\"I hate everyone!\")\n",
        "evaluate_prediction(\"This is a test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LftCNOfxbYy_"
      },
      "outputs": [],
      "source": [
        "embedding_model = tf.keras.Model(\n",
        "    inputs=[input_ids, attention_mask],\n",
        "    outputs=cls_embedding\n",
        ")\n",
        "\n",
        "@tf.function(input_signature=[\n",
        "    tf.TensorSpec(shape=[None, 128], dtype=tf.int32, name='input_word_ids'),\n",
        "    tf.TensorSpec(shape=[None, 128], dtype=tf.int32, name='input_mask')\n",
        "])\n",
        "def serving_fn(input_word_ids, input_mask):\n",
        "    return embedding_model([input_word_ids, input_mask])\n",
        "\n",
        "tf.saved_model.save(\n",
        "    embedding_model,\n",
        "    '/content/drive/MyDrive/eng_models_att2/eng_labse_hate_embeddings',\n",
        "    signatures={\n",
        "        'serving_default': serving_fn.get_concrete_function()\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wchOZicEbY2Y"
      },
      "outputs": [],
      "source": [
        "# loading verification\n",
        "loaded_model = tf.saved_model.load('/content/drive/MyDrive/eng_labse_hate_embeddings')\n",
        "serving_fn = loaded_model.signatures['serving_default']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncO2rUFebjg-"
      },
      "outputs": [],
      "source": [
        "# Test with dummy data\n",
        "test_input = {\n",
        "    'input_word_ids': tf.constant([[1]*128], dtype=tf.int32),\n",
        "    'input_mask': tf.constant([[1]*128], dtype=tf.int32)\n",
        "}\n",
        "embeddings = serving_fn(**test_input)['output_0']\n",
        "print(embeddings.shape)  #  (1, 768)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
